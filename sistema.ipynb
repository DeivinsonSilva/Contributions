{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sistema.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPqsZVIqUTac+G+EDDs8Q/1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeivinsonSilva/Contributions/blob/main/sistema.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDesYQS9FAA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b8fbed-f30d-44c1-f9d8-8126d4023c66"
      },
      "source": [
        "# Libs de NLP\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import Counter \n",
        "from string import punctuation\n",
        "\n",
        "import string\n",
        "from nltk.tokenize.regexp import RegexpTokenizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ7FSG-jD_qz",
        "outputId": "4fca2cab-148b-4eea-c064-47d370c68f42"
      },
      "source": [
        "!pip3 install sumy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from sumy) (3.2.5)\n",
            "Collecting breadability>=0.1.20\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from sumy) (2.23.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from sumy) (0.6.2)\n",
            "Collecting pycountry>=18.2.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.0.2->sumy) (1.15.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from breadability>=0.1.20->sumy) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.7/dist-packages (from breadability>=0.1.20->sumy) (4.2.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (2020.12.5)\n",
            "Building wheels for collected packages: breadability, pycountry\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21680 sha256=b31c9e15ad1dca033e7446285ca62942ad1bd335dcd74c64008280a56abcb1b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746863 sha256=738299a72fe399881f2aad94f2a4ba3f6c3d9c4a7b89968295fc622ee108176e\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\n",
            "Successfully built breadability pycountry\n",
            "Installing collected packages: breadability, pycountry, sumy\n",
            "Successfully installed breadability-0.1.20 pycountry-20.7.3 sumy-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v0jUsiGGcoX"
      },
      "source": [
        "# Sumarizando com Sumy.Luhn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wzT4W-Uj_B1"
      },
      "source": [
        "# Libs de sumarização\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdIBAUhHEFla",
        "outputId": "49e6f70c-37e9-4978-84b0-765870866448"
      },
      "source": [
        "# ... Aqui é realizada a leitura do texto a ser sumarizado...\n",
        "texto = ''\n",
        "entrada = str(input('Selecione o arquivo.txt ou arquivo.doc: '))\n",
        "frases = int(input('Tamanho do texto desejado (em frases): '))\n",
        "arquivo = open(entrada, 'r')\n",
        "for line in arquivo:\n",
        "  texto += line # Varíavel contendo as linhas do texto original\n",
        "\n",
        "# ... Função que carrega o algoritmo de Luhn para sumarizar o texto...\n",
        "def summarizer_luhn(text):\n",
        "  parser = PlaintextParser.from_string(text,Tokenizer(\"portuguese\"))\n",
        "  summarizer_luhn = LuhnSummarizer()\n",
        "  summary_2 = summarizer_luhn(parser.document,frases) # A variável frases possui a quantidade de sentenças informada pelo usuário\n",
        "  luhn_summary=\"\"\n",
        "  for sentence_2 in summary_2:\n",
        "    luhn_summary+=str(sentence_2)  \n",
        "  return (luhn_summary)\n",
        "\n",
        "texto_sumarizado = summarizer_luhn(texto) # Variável que recebe o texto sumarizado\n",
        "\n",
        "# ... Função para o pré-processamento...\n",
        "def preprocess(text):\n",
        "  \n",
        "  # remover pontuações\n",
        "  text   = text.translate(string.punctuation)\n",
        "   \n",
        "  # tokenizar o texto em palavras\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "\n",
        "  # filtrar palavras\n",
        "  tokens = [word for word in tokens\n",
        "            if word not in stopwords       # descartar stopwords\n",
        "                and len(word) > 3          # descartar palavras com menos de 3 caracteres\n",
        "                and not word[0].isdigit()] # descartar tokens contendo apenas numeros\n",
        "  return tokens\n",
        "\n",
        "# ...Interface \n",
        "tam_origi = len(preprocess(texto))\n",
        "tam_resum = len(preprocess(texto_sumarizado))\n",
        "percent = (tam_resum*100)/tam_origi\n",
        "print('Quantidade de palavras:')\n",
        "print('Texto original: {}'.format(tam_origi))\n",
        "print('Texto resumido: {}'.format(tam_resum))\n",
        "print('Tamanho do texto após o resumo (em porcentagem): {:.2f}%'.format(percent))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecione o arquivo.txt ou arquivo.doc: /content/texto61.txt\n",
            "Tamanho do texto desejado (em frases): 7\n",
            "Quantidade de palavras:\n",
            "Texto original: 181\n",
            "Texto resumido: 143\n",
            "Tamanho do texto após o resumo (em porcentagem): 79.01%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}